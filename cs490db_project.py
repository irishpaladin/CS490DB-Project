# -*- coding: utf-8 -*-
"""CS490DB-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RIGem-t7q1RjJLMKuVOygvYESlvCbEDI

```
CS 490DB - Applications in Natural Sciences

Due: Thursday, December 5, 2019 at 11:59 pm

Instructor: Trevor Tomesh

By: Stephanie Irish Paladin

Student id - 200413341

Email: stephanieirishpaladin@gmail.com
```

#Project
In this project, my goal is to predict the price of the houses in King City, USA using the data I got from [Kaggle.com](https://kaggle.com)

##Part A - Determining a proper model

Since I am predicting the price of the houses, I concluded that I will be using Regression analysis. As for what type of algorithm I will be using, I rely on the Scikit-learn algorithm cheat-sheet and it gave me a result of RidgeRegression. The score I got in using the Ridge Regression is not as high as I thought it would be so I tried to use different types of algorithm. I got a better score after using Linear Algorithm with Polynomial feature and so I decided to use that model for this project.

**Polynomial Regression**

Polynomial Regression is a form of linear regression but with a curve. The value of the dependent variable y depends on a function with x raised to the nth power wherein 'n' is the degree of the polynomial and x is an independent variable.

To implement the algorithm, we need to import the following libraries from sklearn:
```
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
```
*Polynomialfeatures* is used in transforming the x values to accommodate the "polynomial feature" of the model.

*LinearRegression* library is used in creating the model, but instead of the original x values, we will be using the transformed x values.

##Part B - Apply the Algorithm
```
1. implement the algorithm to establish the model.

2. Explain as clearly as you can the process involved in creating the model.

3. give any details about how the data needs to be prepared in order to be used with the selected algorithm.
```

**Preparing Data**
"""

# Importing necessary libraries
from google.colab import drive
import pandas as pd
import matplotlib.pyplot as plt
import os
import numpy as np
drive.mount('/content/grive')

#cd grive/My\ Drive/2Year1Term/CS490DB/CS490DB-Project/data

# Retrieving data
data = pd.read_csv("kc_house_data.csv")

data.shape

data.dtypes

"""Seeing the above information, there are few values that needs to be manipulated

1.   date as datatype datetime
2.   bathrooms from float to int
3.   floors from float to int
"""

data['date'] = pd.to_datetime(data['date'])
data['bathrooms'] = data['bathrooms'].astype('int')
data['floors'] = data['floors'].astype('int')

data.id.duplicated

"""Since column 'id' is unique, I will change the index of the Data Frame with the id."""

data.set_index(data.id, inplace=True)

"""The column date and yr_built column can be combined into a single column. I will name it as house_age"""

data["house_age"] = data["date"].dt.year - data['yr_built']

"""The column yr_renovated will be changed to a boolean value to avoid extreeme values in the data."""

data['renovated'] = data['yr_renovated'].apply(lambda yr: 0 if yr == 0 else 1)

"""Luckily, I dont have any missing values in the dataset."""

data.isnull().sum()

"""Since there are no missing values, and so I will start removing unneeded values"""

data.drop('date', axis=1, inplace=True)
data.drop('yr_built', axis=1, inplace=True)
data.drop('id', axis=1, inplace=True)
data.drop('yr_renovated',axis=1, inplace=True)
data.head()

"""**Creating Model**"""

# Import necessary library
import numpy as np
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures 
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split

# Preparing the data
y = data['price'].astype('int')
X = data.drop('price', axis = 1)

"""To test the accuracy of the model, I split the data into two parts:


1.   Train data: Data that will be used to fit to create the model.
2.   Test data: Data that will be used to test the data.
"""

# Splitting train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit and transforms the data to a Polynomial Feature
X_train_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_train)

# Create model
model = LinearRegression().fit(X_train_,y_train)

# Predicting values using the test data
X_test_= PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_test)
pred = model.predict(X_test_)
pred

"""##Part C - Visualize and Explain Your Model
```
1. From the model created in part B, visualize the regression, clustering, classification or reduction.

2. Clearly label any and all important aspects in your visualization.

3. Explain your plot in detail and what the plot means.

4. In the case of a regression provide the equation of fit along with the r-squared values and any other relevant statistical information that supports your model.

```

I used seaborn to plot the relationshib between the predicted and actual data. The diagonal line at the middle is the line of best fit. The nearer the plot is in the line, the more accurate it is.
"""

# Plots the model and the original data

plt.figure(figsize= (6, 6))
plt.title('Comparing Prediction and Actual Data')

sns.regplot(pred, y_test)
plt.xlabel("Predicted Price")

plt.ylabel("Original Price")
plt.show()

# Get the values needed to plot the data
b0 = model.intercept_
b1 = model.coef_[0]
b2 = model.coef_[1]
b3 = model.coef_[2]

print('intercept: ',b0)
print('coef_0: ', b1)
print('coef_1: ',b2)
print('coef_2: ',b3)

"""**Providing statistical information that supports the model.**"""

print("Mean absolute error:", mean_absolute_error(y_test, pred))
rmse = np.sqrt(mean_squared_error(y_test,pred))
print('Mean squared error: ',rmse)
r2 = r2_score(y_test,pred)
print("Coeffients: ", model.coef_)
print("Intercepts:" ,model.intercept_)
print('r2_score: ',r2_score(y_test,pred)*100)

fig, ax = plt.subplots(figsize=(15,9))         # Sample figsize in inches

correlation = data.corr(method='pearson')
columns = correlation.nlargest(19, 'price').index
correlation_map = np.corrcoef(data[columns].values.T)
sns.set(font_scale=0.7)
heatmap = sns.heatmap(correlation_map, annot=True, fmt='.2f', yticklabels=columns.values, xticklabels=columns.values, ax=ax)

plt.show()

"""The model produced a negative and positive coefficienct. Looking at the relationship of the data, the price of the house is directly proportional the sqrft_living and grade of the house. There is a big posibility that the positive coefficient has a relationship with either of the two column.

Meanwhile, house_age had an indirect relationship with the price. Price lowers whenever the age of the house increases. In reality, this, ofcourse, is true especailly for the houses that has not been renovated for a long time. I could infer that this is the value has the most impact with the negative coefficient.

##Part D - Discussion


```
1. Now that you have established a model, consider the implications of that model. Does the model have the power to predict the outcome of future instances? What does it prove?

2. Create a fictional instance to analyze. How well does the model describe the outcome? 

3. Reflect on the potential accuracy of the model both in interpolation and extrapolation. Do you see any potential limitations?
```

**Implication of the model and does it have the power to predict the outcome of  future instances?**

I am confindent that my model is able to predict the future outcome. The score was not high as I thought it would be but considering the number of data, I belive that having 80% score is good.

**Create a fictional instance to analyze. How well does the model describe the outcome?**

Here, I added 50 data in years starting from 50 to 74.5 as the continuation of the given data. I used the model to predict the population. I plot the result together with the original data and see if there's a considerable change in the trend.
"""

# Loading the CSV file into a pandas dataframe.
geese_data2 = pd.read_csv('years2.csv')

x_future = np.array(geese_data2['yrs']).reshape(-1,1)

x_future_= PolynomialFeatures(degree=3, include_bias=False).fit_transform(x_future)
pred = model.predict(x_future_)

# Plotting the original data
plt.scatter(x,y,label = 'Actual data')
plt.scatter(x_future, pred, label = 'Added points',color='g')
xn = np.arange(x.min(),x_future.max()).reshape(-1,1)
plt.plot(xn,b0+(b1*xn)+(b2*xn**2)+(b3*xn**3),color='r')
plt.xlabel('Years')
plt.ylabel('Population')
plt.legend()
plt.show()

"""**Limitation of the Model**

The limitation of the model lies in a fact it is an algorithm. It highly depends on the data that was given into it. To have a good model that requires accuracy, it has to be fed with a lot of data.

##Part E - Summary


**1. Summarize what you have done in your notebook. What was the goal and what were your results?**

The goal of this project is to predict the price of the house based on the features of that house. I have a dataset which contains data of the houses and its price. I cleaned the data by modifying some of the values' data type as well as (combining to wo columns to one). Using the cleaned data, I created a Polynomial regression model which resulted with a an 80$ score which fairly high considering the number of data it has.

**2. Reflect on whether some other analysis would have been better for this data.**

In every model, it is always better to have more data. More data means you have another data to plot. MOreover, more data also means a more accurate model. It's also unfortunate that I only  have little knowledge about machine learning. There are models which I would like to try but unable to do so due to my limited knowledge about that topic.

**3. Finally, what sort of new knowledge were you able to create from your analysis?**

I have learned a lot of about cleaning the data. Cleaning data is not as easy as I imagine it to be. I have to consider a lot of thing before removing or modifying the data. I learn that even just one small change in the data gives a signifficant change in the model.

##References

[1] Choosing the right estimator â€” scikit-learn 0.21.3 documentation. Scikit-learn.org. https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html. Published 2019. [Accessed November 30, 2019].

[2] House Sales in King County, USA. Kaggle.com. https://www.kaggle.com/harlfoxem/housesalesprediction. Published 2019. [Accessed November 30, 2019].
"""